---
title: "Исследование метаданных DNS трафика"
subtitle: "Отчет по практике 4"
author: "IT-life1@yandex.ru"
format: 
  md:
    output-file: README.md
---

## Цель работы

1.  Закрепить практические навыки работы с языком программирования R в контексте анализа сетевых данных\
2.  Отработать применение ключевых функций из экосистемы `tidyverse` для очистки, трансформации и агрегации данных\
3.  Научиться выявлять аномалии и скрытые паттерны в метаданных DNS-трафика

## Исходные данные

1.  Операционная система: Windows 10
2.  Среда разработки: RStudio\
3.  Версия интерпретатора R: 4.5.1

## Ход работы

1.  Импортируем данные DNS – https://storage.yandexcloud.net/dataset.ctfsec/dns.zip\
    Данные были собраны с помощью сетевого анализатора zeek.

2.  Добавим пропущенные данные о структуре данных (назначении столбцов).

3.  Преобразуем данные в столбцах в нужный формат, просмотрим общую структуру данных с помощью функции `glimpse()`.

4.  Сколько участников информационного обмена в сети Доброй Организации?

5.  Какое соотношение участников обмена внутрисети и участников обращений к внешним ресурсам?

6.  Найдем топ-10 участников сети, проявляющих наибольшую сетевую активность.

7.  Найдем топ-10 доменов, к которым обращаются пользователи сети и соответственное количество обращений.

8.  Определим базовые статистические характеристики (функция `summary()`) интервала времени между последовательными обращениями к топ-10 доменам.

9.  Часто вредоносное программное обеспечение использует DNS канал в качестве канала управления, периодически отправляя запросы на подконтрольный злоумышленникам DNS сервер. По периодическим запросам на один и тот же домен можно выявить скрытый DNS канал. Есть ли такие IP адреса в исследуемом датасете?

10. Определим местоположение (страну, город) и организацию-провайдера для топ-10 доменов. Для этого можно использовать сторонние сервисы, например http://ip-api.com (API-эндпоинт – http://ip-api.com/json).

### Шаг 1

Импортируем данные DNS

![](./imgs/img1.png)

Установим необходимые билиотеки

```{r}
library(dplyr)
library(readr)
```

```{r}

dns_raw <- read_delim(
  "Lab4/dns.log",
  delim = "\t",
  col_names = cols,
  comment = "#",
  na = c("-", "", "NA"),
  trim_ws = TRUE,
  show_col_types = FALSE
)

```

### Шаг 2

Добавим пропущенные данные о структуре данных

```{r}
cols <- c(
  "timestamp", "uid", "source_ip", "source_port", "destination_ip",
  "destination_port", "protocol", "transaction_id", "query", "qclass",
  "qclass_name", "qtype", "qtype_name", "rcode", "rcode_name",
  "AA", "TC", "RD", "RA", "Z", "answers", "TTLS", "rejected"
)

dns_raw <- read_delim(
  "Lab4/dns.log",
  delim = "\t",
  col_names = cols,
  comment = "#",
  na = c("-", "", "NA"),
  trim_ws = TRUE,
  show_col_types = FALSE
)

```

### Шаг 3

Преобразуем данные в столбцах в нужный формат

```{r}
dns_clean <- dns_raw %>%
  mutate(
    across(c(timestamp), ~ as.POSIXct(.x, origin = "1970-01-01")),
    across(c(source_port, destination_port, transaction_id, qclass, qtype, rcode), as.numeric)
  )

glimpse(dns_clean)
```

### Шаг 4

Найдем число участников информационного обмена в сети Доброй Организации

```{r}
all_ips <- dns_clean %>%
  select(source_ip, destination_ip) %>%
  unlist() %>%
  unique()

length(all_ips)
```

### Шаг 5

Найдем соотношение участников обмена внутрисети и участников обращений к внешним ресурсам

```{r}
private_pattern <- "^(10\\.|192\\.168\\.|172\\.(1[6-9]|2[0-9]|3[0-1])\\.)"
internal_count <- sum(grepl(private_pattern, all_ips))
external_count <- length(all_ips) - internal_count
internal_count / external_count
```

### Шаг 6

Найдем топ-10 участников сети, проявляющих наибольшую сетевую активность

```{r}
dns_clean %>%
  count(source_ip, sort = TRUE) %>%
  head(10)
```

### Шаг 7

Найдем топ-10 доменов, к которым обращаются пользователи сети и соответственное количество обращений

```{r}
top_domains <- dns_clean %>%
  count(query, sort = TRUE) %>%
  head(10)

top_domains
```

### Шаг 8

Определим базовые статистические характеристики (функция `summary()`) интервала времени между последовательными обращениями к топ-10 доменам

```{r}
library(purrr)

interval_stats <- top_domains$query %>%
  map_dfr(~ {
    df <- dns_clean %>% filter(query == .x) %>% arrange(timestamp)
    if (nrow(df) < 2) return(NULL)
    diffs <- diff(as.numeric(df$timestamp))
    s <- summary(diffs)
    tibble(
      Domain = .x,
      Min = s["Min."],
      Q1 = s["1st Qu."],
      Median = s["Median"],
      Mean = s["Mean"],
      Q3 = s["3rd Qu."],
      Max = s["Max."]
    )
  })

interval_stats
```

### Шаг 9

Определим скрытый DNS канал

```{r}
# Инициализируем пустой tibble с нужной структурой
simple_suspicious <- tibble(
  source_ip = character(),
  request_count = integer(),
  time_diffs = list(),
  avg_interval = numeric(),
  std_dev = numeric(),
  cv = numeric(),
  is_periodic = logical(),
  domain = character()
)

# Теперь можно безопасно проверить количество строк
if (nrow(simple_suspicious) == 0) {
  
  for (domain in top_domains$query) {  # ← убедитесь, что top_domains — tibble!
    domain_data <- dns_clean %>% 
      filter(query == domain, !is.na(timestamp))
    
    if (nrow(domain_data) >= 3) {
      result <- domain_data %>%
        group_by(source_ip) %>%
        filter(n() >= 3) %>%
        arrange(timestamp) %>%
        summarise(
          request_count = n(),
          time_diffs = list(diff(as.numeric(timestamp))),
          .groups = "drop"
        ) %>%
        filter(lengths(time_diffs) > 0) %>%
        mutate(
          avg_interval = map_dbl(time_diffs, ~ mean(.x, na.rm = TRUE)),
          std_dev = map_dbl(time_diffs, ~ sd(.x, na.rm = TRUE)),
          cv = ifelse(avg_interval > 0, std_dev / avg_interval, NA),
          is_periodic = !is.na(cv) & cv < 1.5,
          domain = domain
        ) %>%
        filter(is_periodic)
      
      if (nrow(result) > 0) {
        simple_suspicious <- bind_rows(simple_suspicious, result)
      }
    }
  }
}

# Вывод результата
simple_suspicious
```

### Шаг 10

Определим местоположение (страну, город) и организацию-провайдера для топ-10 доменов.

```{r}
# Подключаем пакеты
library(dplyr)
library(tidyr)
library(tibble)
library(httr)
library(jsonlite)

top_domains <- dns_clean %>% count(query, sort = TRUE) %>% head(10)
# Функция для проверки частного IP
is_private_ip <- function(ip) {
  grepl("^(10\\.|192\\.168\\.|172\\.(1[6-9]|2[0-9]|3[0-1])\\.)", ip)
}

# Функция геолокации
get_geo_info <- function(ip) {
  if (is_private_ip(ip)) {
    return(tibble(country = "Частный IP", city = "Частный IP", isp = "Частный IP"))
  }
  url <- paste0("http://ip-api.com/json/", ip)
  resp <- GET(url)
  if (status_code(resp) != 200) {
    return(tibble(country = "Ошибка API", city = "Ошибка API", isp = "Ошибка API"))
  }
  data <- fromJSON(content(resp, "text"), simplifyVector = TRUE)
  if (!is.null(data$status) && data$status == "success") {
    tibble(
      country = if (is.null(data$country)) "—" else data$country,
      city = if (is.null(data$city)) "—" else data$city,
      isp = if (is.null(data$isp)) "—" else data$isp
    )
  } else {
    tibble(country = "Не определено", city = "Не определено", isp = "Не определено")
  }
}

domain_geo <- top_domains %>%
  rowwise() %>%
  mutate(
    ip = dns_clean %>%
      filter(query == .data$query) %>%
      pull(destination_ip) %>%
      first(),
    geo = list(get_geo_info(ip))
  ) %>%
  ungroup() %>%
  unnest_wider(geo) %>%
  select(domain = query, ip_address = ip, country, city, isp)

domain_geo
```