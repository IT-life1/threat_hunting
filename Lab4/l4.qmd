---
title: "Практическая работа 004"
author: "IT-life1@yandex.ru"
format: 
  md:
    output-file: README.md
---

## Цель работы

1.  Закрепить практические навыки работы с языком программирования R в контексте анализа сетевых данных\
2.  Отработать применение ключевых функций из экосистемы `tidyverse` для очистки, трансформации и агрегации данных\
3.  Научиться выявлять аномалии и скрытые паттерны в метаданных DNS-трафика

## Исходные данные

1.  Операционная система: Windows 10
2.  Среда разработки: RStudio\
3.  Версия интерпретатора R: 4.5.1

## Ход работы

1.  Импортируем данные DNS – https://storage.yandexcloud.net/dataset.ctfsec/dns.zip\
    Данные были собраны с помощью сетевого анализатора zeek.

2.  Добавим пропущенные данные о структуре данных (назначении столбцов).

3.  Преобразуем данные в столбцах в нужный формат, просмотрим общую структуру данных с помощью функции `glimpse()`.

4.  Сколько участников информационного обмена в сети Доброй Организации?

5.  Какое соотношение участников обмена внутрисети и участников обращений к внешним ресурсам?

6.  Найдем топ-10 участников сети, проявляющих наибольшую сетевую активность.

7.  Найдем топ-10 доменов, к которым обращаются пользователи сети и соответственное количество обращений.

8.  Определим базовые статистические характеристики (функция `summary()`) интервала времени между последовательными обращениями к топ-10 доменам.

9.  Часто вредоносное программное обеспечение использует DNS канал в качестве канала управления, периодически отправляя запросы на подконтрольный злоумышленникам DNS сервер. По периодическим запросам на один и тот же домен можно выявить скрытый DNS канал. Есть ли такие IP адреса в исследуемом датасете?

10. Определим местоположение (страну, город) и организацию-провайдера для топ-10 доменов. Для этого можно использовать сторонние сервисы, например http://ip-api.com (API-эндпоинт – http://ip-api.com/json).

### Шаг 1

Импортируем данные DNS

![](./imgs/img1.png)

Установим необходимые билиотеки

```{r}
library(dplyr)
library(readr)
```

```{r}

dns_raw <- read_delim(
  "dns.log",
  delim = "\t",
  col_names = cols,
  comment = "#",
  na = c("-", "", "NA"),
  trim_ws = TRUE,
  show_col_types = FALSE
)

```

### Шаг 2

Добавим пропущенные данные о структуре данных

```{r}
cols <- c(
  "timestamp", "uid", "source_ip", "source_port", "destination_ip",
  "destination_port", "protocol", "transaction_id", "query", "qclass",
  "qclass_name", "qtype", "qtype_name", "rcode", "rcode_name",
  "AA", "TC", "RD", "RA", "Z", "answers", "TTLS", "rejected"
)

dns_raw <- read_delim(
  "dns.log",
  delim = "\t",
  col_names = cols,
  comment = "#",
  na = c("-", "", "NA"),
  trim_ws = TRUE,
  show_col_types = FALSE
)

```

### Шаг 3

Преобразуем данные в столбцах в нужный формат

```{r}
dns_clean <- dns_raw %>%
  mutate(
    across(c(timestamp), ~ as.POSIXct(.x, origin = "1970-01-01")),
    across(c(source_port, destination_port, transaction_id, qclass, qtype, rcode), as.numeric)
  )

glimpse(dns_clean)
```

### Шаг 4

Найдем число участников информационного обмена в сети Доброй Организации

```{r}
all_ips <- dns_clean %>%
  select(source_ip, destination_ip) %>%
  unlist() %>%
  unique()

length(all_ips)
```

### Шаг 5

Найдем соотношение участников обмена внутрисети и участников обращений к внешним ресурсам

```{r}
library(dplyr)

participants <- unique(c(dns_clean$source_ip, dns_clean$destination_ip))
is_internal <- function(ip) {
  grepl("^192\\.168\\.|^10\\.|^172\\.(1[6-9]|2[0-9]|3[0-1])\\.", ip)
}

internal_ips <- participants[is_internal(participants)]
external_ips <- participants[!is_internal(participants)]

internal_count <- length(internal_ips)
external_count <- length(external_ips)

internal_count / external_count
```

### Шаг 6

Найдем топ-10 участников сети, проявляющих наибольшую сетевую активность

```{r}
dns_clean %>%
  count(source_ip, sort = TRUE) %>%
  head(10)
```

### Шаг 7

Найдем топ-10 доменов, к которым обращаются пользователи сети и соответственное количество обращений

```{r}
top_domains <- dns_clean %>%
  filter(!is.na(query)) %>%
  count(query, name = "count") %>%
  arrange(desc(count)) %>%
  head(10)
top_domains
```

### Шаг 8

Определим базовые статистические характеристики (функция `summary()`) интервала времени между последовательными обращениями к топ-10 доменам

```{r}
library(purrr)

interval_stats <- top_domains$query %>%
  map_dfr(~ {
    df <- dns_clean %>% filter(query == .x) %>% arrange(timestamp)
    if (nrow(df) < 2) return(NULL)
    diffs <- diff(as.numeric(df$timestamp))
    s <- summary(diffs)
    tibble(
      Domain = .x,
      Min = s["Min."],
      Q1 = s["1st Qu."],
      Median = s["Median"],
      Mean = s["Mean"],
      Q3 = s["3rd Qu."],
      Max = s["Max."]
    )
  })

interval_stats
```

### Шаг 9

Определим скрытый DNS канал

```{r}
simple_suspicious <- tibble(
  source_ip = character(),
  request_count = integer(),
  time_diffs = list(),
  avg_interval = numeric(),
  std_dev = numeric(),
  cv = numeric(),
  is_periodic = logical(),
  domain = character()
)

if (nrow(simple_suspicious) == 0) {
  
  for (domain in top_domains$query) {
    domain_data <- dns_clean %>% 
      filter(query == domain, !is.na(timestamp))
    
    if (nrow(domain_data) >= 3) {
      result <- domain_data %>%
        group_by(source_ip) %>%
        filter(n() >= 3) %>%
        arrange(timestamp) %>%
        summarise(
          request_count = n(),
          time_diffs = list(diff(as.numeric(timestamp))),
          .groups = "drop"
        ) %>%
        filter(lengths(time_diffs) > 0) %>%
        mutate(
          avg_interval = map_dbl(time_diffs, ~ mean(.x, na.rm = TRUE)),
          std_dev = map_dbl(time_diffs, ~ sd(.x, na.rm = TRUE)),
          cv = ifelse(avg_interval > 0, std_dev / avg_interval, NA),
          is_periodic = !is.na(cv) & cv < 1.5,
          domain = domain
        ) %>%
        filter(is_periodic)
      
      if (nrow(result) > 0) {
        simple_suspicious <- bind_rows(simple_suspicious, result)
      }
    }
  }
}

simple_suspicious
```

### Шаг 10

Определим местоположение (страну, город) и организацию-провайдера для топ-10 доменов.

```{r}
library(httr)
library(jsonlite)
library(dplyr)

lookup_domain_ip <- function(domain) {
  url <- paste0("https://dns.google/resolve?name=", domain, "&type=A")
  
  resp <- tryCatch(fromJSON(url), error = function(e) return(NA))
  
  if (!is.null(resp$Answer)) {
    ip <- resp$Answer$data[resp$Answer$type == 1] 
    if (length(ip) > 0) return(ip[1])
  }
  return(NA)
}

get_ip_info <- function(ip) {
  if (is.na(ip)) return(data.frame(query = NA, country = NA, city = NA, org = NA))
  
  url <- paste0("http://ip-api.com/json/", ip)
  resp <- tryCatch(GET(url), error = function(e) NULL)
  
  if (!is.null(resp) && resp$status_code == 200) {
    parsed <- tryCatch(content(resp, as = "parsed"), error = function(e) NULL)
    return(data.frame(query = parsed$query, country = parsed$country, city = parsed$city, org = parsed$org))
  }
  
  return(data.frame(query = ip, country = NA, city = NA, org = NA))
}

valid_domains <- top_domains %>%
  filter(grepl("\\.", query)) %>% 
  slice(1:10)

top_domains_with_geo <- valid_domains$query %>%
  lapply(function(domain) {
    ip <- lookup_domain_ip(domain)
    info <- get_ip_info(ip)
    cbind(domain = domain, ip = ip, info)
  }) %>% 
  bind_rows()

top_domains_with_geo
```