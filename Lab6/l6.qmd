---
title: "Практическая работа 006"
author: "IT-life1@yandex.ru"
format:
  md:
    output-file: README.md
---

# Исследование вредоносной активности в домене Windows

## Цель работы

1. Закрепить навыки исследования данных журнала Windows Active Directory
2. Изучить структуру журнала системы Windows Active Directory
3. Зекрепить практические навыки использования языка программирования R для обработки данных
4. Закрепить знания основных функций обработки данных экосистемы tidyverse языка R

## Исходные данные

1.  Операционная система: Windows 10
2.  Среда разработки: RStudio
3.  Версия интерпретатора R: 4.5.1

## Ход работы

1.  Импортируем данные – https://storage.yandexcloud.net/iamcth-data/dataset.tar.gz. и https://learn.microsoft.com/en-us/windows-server/identity/adds/
plan/appendix-l--events-to-monitor; подготовим данные для дальнейшего анализа.

2.  Проведем анализ датасетов с точками доступа: 
    2.1 Раскройте датафрейм избавившись от вложенных датафреймов. Для
    обнаружения таких можно использовать функцию dplyr::glimpse() , а для
    раскрытия вложенности – tidyr::unnest() . Обратите внимание, что при
    раскрытии теряются внешние названия колонок – это можно предотвратить
    если использовать параметр tidyr::unnest(..., names_sep = ).
    2.2 Минимизируйте количество колонок в датафрейме – уберите колоки с
    единственным значением параметра.
    2.3 Какое количество хостов представлено в данном датасете?
    2.4 Подготовьте датафрейм с расшифровкой Windows Event_ID, приведите типы
    данных к типу их значений.
    2.5 Есть ли в логе события с высоким и средним уровнем значимости? Сколько их?


### Шаг 1

#### Импортируем исходный датасет

![](./imgs/img1.png)

Установим необходимые библиотеки
```{r}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(readr)
library(janitor)
library(jsonlite)
library(dplyr)
library(stringr)
tar_path <- "dataset.tar.gz"

temp_dir <- tempdir()
untar(
  tarfile = tar_path,
  exdir = temp_dir
)

json_files <- list.files(temp_dir, pattern = "\\.json$", full.names = TRUE, recursive = TRUE)
json_path <- file.path(temp_dir, "caldera_attack_evals_round1_day1_2019-10-20201108.json")
events <- stream_in(file(json_path), verbose = FALSE)
```

#### Подготовим и импортируем коды журнала Windows
```{r}
library(xml2)
library(rvest)
library(dplyr)
library(stringr)
library(tidyr)
library(janitor)

webpage_url <- "https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/appendix-l--events-to-monitor"
webpage <- read_html(webpage_url)
event_df_raw <- html_table(webpage)[[1]]
event_df <- event_df_raw %>%
  mutate(
    `Current Windows Event ID` = as.numeric(`Current Windows Event ID`),
    `Legacy Windows Event ID` = as.numeric(`Legacy Windows Event ID`),
    `Potential criticality` = as.character(`Potential Criticality`),
    `Event Summary` = as.character(`Event Summary`)
  )
head(event_df,10)


```

### Шаг 2

#### Посмотрим, что внутри датасета
```{r}
glimpse(events)
```

#### Подготовим датасет для дальнейшей работы
```{r}
events1 <- events %>%
  tidyr::unnest_wider(event, names_sep = "")
```

```{r}
events2 <- events1 %>%
  tidyr::unnest_wider(winlog, names_sep = "")
```

```{r}
events3 <- events2 %>%
  tidyr::unnest_wider(host, names_sep = "") %>%
  tidyr::unnest_wider(agent, names_sep = "") %>%
  tidyr::unnest_wider(ecs, names_sep = "")
```

#### Посмотрим на итоговые поля в распаршеном датасете
```{r}
names(events3)
```

#### Минимизируем количество колонок в датафрейме, убрав колонки с единственным значением параметра.
```{r}
events_min <- events3 %>%
  select(-`@metadata`) %>%
  select(where(~ dplyr::n_distinct(.x) > 1))

glimpse(events_min)


```

#### Посчитаем количество хостов представлено в данном датасете.
```{r}
hosts_count <- events_min %>%
  summarise(n_hosts = n_distinct(winlogcomputer_name))

hosts_count

```

#### Посмотрим на список уникальных хостов.
```{r}
hosts_list <- events_min %>%
  distinct(winlogcomputer_name)

hosts_list

```

#### Подготовим датафрейм с расшифровкой Windows Event_ID.
```{r}
event_df_clean <- event_df_raw %>%
  janitor::clean_names() %>% 
  transmute(
    event_id        = as.numeric(current_windows_event_id),
    legacy_event_id = as.numeric(legacy_windows_event_id),
    criticality     = as.character(potential_criticality),
    summary         = as.character(event_summary)
  ) %>%
  filter(!is.na(event_id))

glimpse(event_df_clean)
head(event_df_clean, 10)
```

#### Соединими два датафрейма в один.
```{r}
events_joined <- events_min %>%
  left_join(event_df_clean, by = c("eventcode" = "event_id"))

events_joined %>%
  select(`@timestamp`, winlogcomputer_name, eventcode, criticality, summary) %>%
  head(10)
```

#### Проверим есть ли в логе события с высоким и средним уровнем значимости и их количество.
```{r}
severity_stats <- events_joined %>%
  filter(!is.na(criticality)) %>%
  count(criticality, sort = TRUE)

severity_stats

```

##### Как видим из предыдущего результата в датафрейме есть только LOW критичные события.
```{r}
high_events <- events_joined %>%
  filter(str_detect(criticality, regex("high", ignore_case = TRUE)))

medium_events <- events_joined %>%
  filter(str_detect(criticality, regex("medium", ignore_case = TRUE)))

n_high   <- nrow(high_events)
n_medium <- nrow(medium_events)

n_high
n_medium

```